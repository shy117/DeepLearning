# Lanenet+YoloV5的车道线检测与地标识别

# （一）Lanenet车道线检测

### 一. Lanenet环境部署

- 贴出来我这里的环境配置（**未使用anaconda**）

  ```
  操作系统：windows10
  IDE：Pycharm
  python版本：Pyhon3.7
  pytorch版本：torch 1.7.1
  cuda版本：11.0
  显卡：RTX 2060
  ```

  (Cuda和pytorch这里安装这里不在重复，需要去根据对自己GPU的版本，并且根据Cuda版本去pytorch官网安装对应Cuda与cudnn版本的pytorch)

- 配置环境

  ```31
  pip install setuptools
  pip install numpy
  pip install matplotlib
  pip install pandas
  pip install scipy
  pip install seaborn
  pip install opencv-python
  pip install tqdm
  pip install pillow
  pip install tensorboard
  pip install pyyaml
  pip install pandas
  pip install scikit-image
  pip install Cython
  pip install thop
  pip install torch
  pip install torchvision
  ```

### 二.数据集收集以及标注

#### 1.通过视频获取视频帧，进行数据标注

- 新建video2jpg.py将视频转换成图片帧，为后续标注提供数据集

  ```python
  # 导入所需要的库
  import cv2
  import numpy as np
    
  # 定义保存图片函数
  # image:要保存的图片名字
  # addr；图片地址与相片名字的前部分
  # num: 相片，名字的后缀。int 类型
  def save_image(image,addr,num):
    address = addr + str(num)+ '.jpg'
    cv2.imwrite(address,image)
    
  # 读取视频文件
  videoCapture = cv2.VideoCapture("222.mp4")
    
  #读帧
  success, frame = videoCapture.read()
  i = 0
  timeF = 12
  j=0
  while success :
    i = i + 1
    if (i % timeF == 0):
      j = j + 1
      save_image(frame,'output',j)
      print('save image:',i)
    success, frame = videoCapture.read()
  ```

#### 二.数据集标注

##### #1.安装labelme(使用anaconda)

[Windows下的labelme数据标注工具安装教程_volcano_Lin 的博客-CSDN博客_labelme安装](https://blog.csdn.net/qq_38451119/article/details/83036495)

##### #2.安装labelme（python）

```
pip install pyqt5
pip install labelme
```

##### 启动labelme

- win+r输入CMD启动 CMD
- 输入labelme启动

![image-20210427225107137](方案.assets/image-20210427225107137.png)

##### 对数据集进行标注（<u>**注：这里只是提供一种方法，使用多边形进行标注可能效果更佳**</u>）

- 选择左侧Open Dir 选中对应的文件夹
- 在上方的Edit中下选择CreatLine选择

![image-20210427232903419](方案.assets/image-20210427232903419.png)

- 进行车道线进行标注即可

![image-20210427234124516](方案.assets/image-20210427234124516.png)

#### 三.标注xml文件转换

新建xml2json.py文件将上一步生成的xml转换为我们所需要的数据

```python
import os,glob
LabelPaths = glob.glob('整合/*.json') #相对路径记得自行修改

for LabelPath in LabelPaths:
	print(LabelPath)
	Name = os.path.basename(LabelPath).split('.')[0]
	cmd = 'labelme_json_to_dataset {0} -o {1}'.format(LabelPath, Name)
	os.system(cmd)
```

获取到如下

![image-20210428005041628](方案.assets/image-20210428005041628.png)

![image-20210428005105885](方案.assets/image-20210428005105885-1619542404731.png)

- 文件注释
  - img.png为对应图片原图
  - label.png为之前刚刚标记好的车道线
  - label_names.txt为之前标记车道线时候所输入的名称
  - label_viz.png为混合之后图片

#### 四.数据信息转换为灰度图

- 新建json2grey.py将数据信息转换为训练所用的数据集，并且将上述数据信息统一放到新建的一个annotations文件夹中

```python
import cv2
from skimage import measure, color
from skimage.measure import regionprops
import numpy as np
import os
import copy
 
def skimageFilter(gray):
    binary_warped = copy.copy(gray)
    binary_warped[binary_warped > 0.1] = 255
    gray = (np.dstack((gray, gray, gray))*255).astype('uint8')
    labels = measure.label(gray[:, :, 0], connectivity=1)
    dst = color.label2rgb(labels,bg_label=0, bg_color=(0,0,0))
    gray = cv2.cvtColor(np.uint8(dst*255), cv2.COLOR_RGB2GRAY)
    return binary_warped, gray
 
def moveImageTodir(path,targetPath,name):
    if os.path.isdir(path):
        image_name = "image/"+str(name)+".png"  
        binary_name = "gt_image_binary/"+str(name)+".png"
        instance_name = "gt_image_instance/"+str(name)+".png"
        train_rows = image_name + " " + binary_name + " " + instance_name + "\n"
        origin_img = cv2.imread(path+"/img.png")
        origin_img = cv2.resize(origin_img, (1280,720))
        cv2.imwrite(targetPath+"/"+image_name, origin_img)
        img = cv2.imread(path+'/label.png')
        img = cv2.resize(img, (1280,720))
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        binary_warped, instance = skimageFilter(gray)
        cv2.imwrite(targetPath+"/"+binary_name, binary_warped)
        cv2.imwrite(targetPath+"/"+instance_name, instance)
        print("success create data name is : ", train_rows)
        return train_rows
    return None
 
if __name__ == "__main__":
    count = 0
    with open("E:/road/sun/train.txt", 'w+') as file:     #路径替换成自己的相对路径或者绝对路径
        #for images_dir in os.listdir("./images"):
            dir_name = os.path.join("E:/road/sun/annotations")   #路径替换成自己的相对路径或者绝对路径
            for annotations_dir in os.listdir(dir_name):
                json_dir = os.path.join(dir_name, annotations_dir)
                if os.path.isdir(json_dir):
                    train_rows = moveImageTodir(json_dir, "E:/road/sun", str(count).zfill(4)) #路径替换
                    file.write(train_rows)
                    count += 1
```

![image-20210428012647487](方案.assets/image-20210428012647487.png)

<center style="color:#C0C0C0;text-decoration:underline">当前目录结构</center>

#### 至此车道线数据集准备完毕

### 三.使用pytorch-lanenet学习框架进行学习

- Pytorch-lanenet目录结构

  ```
  ├── Pytorch-lanenet
  │   ├── checkpoints  #训练完成权重输出
  │   ├── data  	#数据集
  		  ├──source_image #源图像
  		  ├──training_data_example #训练图像
  		  				├──gt_image_binary
  		  				├──gt_image_instance
  		  				├──image
  		  				├──train.txt
  		  				├──val.txt
  		  ├──tusimple_test_image #测试图像
  │   ├── model  
  │   ├── average_meter.py
  │   ├──	dataloader.py
  │   ├── test.py   #测试权重
  │   ├──	train.py  #训练权重
  ```

- 将上一步转换出来的数据集放入到data/training_data_example下的对应文件夹从train.txt中随机选几十条放入到val.txt中作为验证集所使用

- train.py简介：

  ```python
  # DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
  DEVICE = 'cuda' #选择设备为显卡
  os.makedirs('checkpoints', exist_ok=True) #生成checkpoints进行存放训练出来的权重
  logging.basicConfig(filename='log.txt', filemode='w', level=logging.WARNING, format='%(asctime)s %(message)s', datefmt='%Y-%m-%d-%H:%M:%S')
  #输出训练日志
  if __name__ == '__main__':
  	dataset = 'data\\training_data_example'   #数据集位置
  	epochs = 1000		#重复次数，重复次数越多效果越好
  	bs = 1		#batch size大小 batch size越大一次训练所选的样本数变大，效果越好
  	lr = 0.0005 #学习率
  	lr_decay = 0.5
  	lr_decay_epoch = 250
  	val_per_epoch = 1
  	save_per_epoch = 250 #保存权重的次数
  ```

- 执行python train.py开始对标注的数据集训练

  - 训练过程中<u>**可能**</u>会遇到这个问题

    ![image-20210428175544781](方案.assets/image-20210428175544781.png)

    我们进入到显示对应报错的dataloader.py文件中进行修改 n_label为10

    ```python
    class LaneDataSet(Dataset):
    	def __init__(self, dataset, n_labels=10, stage=None):
    		self._gt_img_list = []
    		self._gt_label_binary_list = []
    		self._gt_label_instance_list = []
    		self.n_labels = n_labels
    		if stage == 'train':
    			self.transform = train_transform
    		else:
    			self.transform = valid_transform
    		with open(dataset, 'r') as file:
    			for _info in file:
    				info_tmp = _info.strip(' ').split()
    				self._gt_img_list.append(info_tmp[0])
    				self._gt_label_binary_list.append(info_tmp[1])
    				self._gt_label_instance_list.append(info_tmp[2])
    		assert len(self._gt_img_list) == len(self._gt_label_binary_list) == len(self._gt_label_instance_list)
    ```


### 四.使用训练好的权重进行测试

- python test.py使用训练好的权重进行检测

  - test.py数据设置

  ```python
  if __name__ == '__main__':
  	dataset = 'data/training_data_example'
  	val_dataset_file = os.path.join(dataset, 'train.txt')
  	val_dataset = LaneDataSet(val_dataset_file, stage = 'val')
  	val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)
  	model = torch.load('checkpoints/1000.pth', map_location=DEVICE)
  	model.eval()
  	for batch_idx, (image_data, binary_label, instance_label) in enumerate(val_loader):
  		image_data, binary_label, instance_label = image_data.to(DEVICE),binary_label.type(torch.FloatTensor).to(DEVICE),instance_label.to(DEVICE)
  		with torch.set_grad_enabled(False):
  			# 预测，并可视化
  			net_output = model(image_data)
  			seg_logits = net_output["seg_logits"].cpu().numpy()[0]
  			# 背景为（0~50）黄色线为（51~200），白色线为（201~255）
  			result = (np.argmax(seg_logits, axis=0)*127).astype(np.uint8)       # 此处背景是0，黄色线是127，白色线是254
  			cv2.imwrite(os.path.join(save_folder, '{0:04d}.png'.format(batch_idx)), result)
  			# fig, axs = plt.subplots(1,2)
  			# axs[0].imshow(image_data.cpu().numpy()[0,0])
  			# axs[1].imshow(result)
  			# plt.show()
  ```

- 输出的结果会在1lanenet文件夹下生成一个seg_result文件夹，其中为我们权重测试完显示结果的图片存放的地方，为了我们后续测试评分做准备

***<u>Lanenet车道线检测至此完成</u>***

# （二）YoloV5进行地标检测

### 一.环境配置

- 在安装上述完包之后还需要在CMD安装pycocotools

  ```
  pip install pycocotools
  ```

  - 在yoloV5中需要pycocotools模块，COCO是一个大型的图像数据集，用于目标检测、分割、人的关键点检测、素材分割和标题生成，在python中用COCO数据集需要安装pycocotools。但是在windows环境下无法直接通过pip安装pycocotools，安装方法如下：
    先安装Visual C++ 2015 build tools：[Microsoft Visual C++ Build Tools 2015](http://go.microsoft.com/fwlink/?LinkId=691126)，安装好后，在cmd中执行下面命令进行安装

  ```
  pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI
  ```

### 二.yolov5的准备工作

- #### YoloV5框架准备

  - 首先从github上下载下来[YOLOv5](https://github.com/ultralytics/yolov5)，这里改名为yolov5-master-lanenet。然后在data目录下新建Annotations, images, ImageSets, labels 四个文件夹。
    其中images存放的是原始的图片数据集，Annotations存放的是标记后生成的xml文件，labels存放的是保存标记内容的txt文件，ImageSets存放的是训练数据集和测试数据集的分类情况。

  ```
  ├── data
  │   ├── Annotations  进行 detection 任务时的标签文件，xml 形式，文件名与图片名一一对应
  │   ├── images  存放 .jpg 格式的图片文件
  │   ├── ImageSets  存放的是分类和检测的数据集分割文件，包含train.txt, val.txt,trainval.txt,test.txt
  │   ├── labels  存放label标注信息的txt文件，与图片一一对应
  
  ├── ImageSets(train，val，test建议按照8：1：1比例划分)
  │   ├── train.txt  写着用于训练的图片名称
  │   ├── val.txt  写着用于验证的图片名称
  │   ├── trainval.txt  train与val的合集
  │   ├── test.txt  写着用于测试的图片名称
  ```

  <center style="color:#C0C0C0;text-decoration:underline">数据集目录结构</center>

- #### 数据集的准备

  - 可以使用上述文所提到的labelme进行标注，也可以使用较为友好UI的精灵标记助手来辅助我们标记

    下载地址：http://www.jinglingbiaozhu.com/

    参考博文：https://blog.csdn.net/youmumzcs/article/details/79657132

  - 数据集标记好后，将原始图片数据集放到images文件夹中，如图所示![image-20210428233459320](方案.assets/image-20210428233459320.png)

  - 输出的标注文件XML保存在Annotations中![image-20210428233553084](方案.assets/image-20210428233553084.png)

  - <u>***注：选择导出格式时候必须选择pascal—voc导出XML直接选择XML会在后面无法读取到标注的信息***</u>

![image-20210428233946612](方案.assets/image-20210428233946612.png)

- #### 构建数据集

  在yolov5-master-lanenet的根目录下新建一个文件makeTxt.py，代码如下

```python
import os
import random


trainval_percent = 0.9
train_percent = 0.9
xmlfilepath = 'data/Annotations'   #数据集位置
txtsavepath = 'data/ImageSets'	 #图片位置
total_xml = os.listdir(xmlfilepath)

num = len(total_xml)
list = range(num)
tv = int(num * trainval_percent)
tr = int(tv * train_percent)
trainval = random.sample(list, tv)
train = random.sample(trainval, tr)

ftrainval = open('data/ImageSets/trainval.txt', 'w')
ftest = open('data/ImageSets/test.txt', 'w')
ftrain = open('data/ImageSets/train.txt', 'w')
fval = open('data/ImageSets/val.txt', 'w')

for i in list:
    name = total_xml[i][:-4] + '\n'
    if i in trainval:
        ftrainval.write(name)
        if i in train:
            ftrain.write(name)
        else:
            fval.write(name)
    else:
        ftest.write(name)

ftrainval.close()
ftrain.close()
fval.close()
ftest.close()
```

​	再在根目录下新建一个voc_label.py，代码如下，classes=[……] 中填入的一定要是自己在数据集中所标注的类别名称，标记了几个类别就填写几个类别名，填写错误的话会造成读取不出xml文件里的标注信息。

```python
# xml解析包
import xml.etree.ElementTree as ET
import pickle
import os
# os.listdir() 方法用于返回指定的文件夹包含的文件或文件夹的名字的列表
from os import listdir, getcwd
from os.path import join


sets = ['train', 'test', 'val']
classes = ['', '',''] #填入刚刚标记数据集的类别名称


# 进行归一化操作
def convert(size, box): # size:(原图w,原图h) , box:(xmin,xmax,ymin,ymax)
    dw = 1./size[0]     # 1/w
    dh = 1./size[1]     # 1/h
    x = (box[0] + box[1])/2.0   # 物体在图中的中心点x坐标
    y = (box[2] + box[3])/2.0   # 物体在图中的中心点y坐标
    w = box[1] - box[0]         # 物体实际像素宽度
    h = box[3] - box[2]         # 物体实际像素高度
    x = x*dw    # 物体中心点x的坐标比(相当于 x/原图w)
    w = w*dw    # 物体宽度的宽度比(相当于 w/原图w)
    y = y*dh    # 物体中心点y的坐标比(相当于 y/原图h)
    h = h*dh    # 物体宽度的宽度比(相当于 h/原图h)
    return (x, y, w, h)    # 返回 相对于原图的物体中心点的x坐标比,y坐标比,宽度比,高度比,取值范围[0-1]


# year ='2012', 对应图片的id（文件名）
def convert_annotation(image_id):
    '''
    将对应文件名的xml文件转化为label文件，xml文件包含了对应的bunding框以及图片长款大小等信息，
    通过对其解析，然后进行归一化最终读到label文件中去，也就是说
    一张图片文件对应一个xml文件，然后通过解析和归一化，能够将对应的信息保存到唯一一个label文件中去
    labal文件中的格式：calss x y w h　　同时，一张图片对应的类别有多个，所以对应的ｂｕｎｄｉｎｇ的信息也有多个
    '''
    # 对应的通过year 找到相应的文件夹，并且打开相应image_id的xml文件，其对应bund文件
    in_file = open('data/Annotations/%s.xml' % (image_id), encoding='utf-8')
    # 准备在对应的image_id 中写入对应的label，分别为
    # <object-class> <x> <y> <width> <height>
    out_file = open('data/labels/%s.txt' % (image_id), 'w', encoding='utf-8')
    # 解析xml文件
    tree = ET.parse(in_file)
    # 获得对应的键值对
    root = tree.getroot()
    # 获得图片的尺寸大小
    size = root.find('size')
    # 如果xml内的标记为空，增加判断条件
    if size != None:
        # 获得宽
        w = int(size.find('width').text)
        # 获得高
        h = int(size.find('height').text)
        # 遍历目标obj
        for obj in root.iter('object'):
            # 获得difficult ？？
            difficult = obj.find('difficult').text
            # 获得类别 =string 类型
            cls = obj.find('name').text
            # 如果类别不是对应在我们预定好的class文件中，或difficult==1则跳过
            if cls not in classes or int(difficult) == 1:
                continue
            # 通过类别名称找到id
            cls_id = classes.index(cls)
            # 找到bndbox 对象
            xmlbox = obj.find('bndbox')
            # 获取对应的bndbox的数组 = ['xmin','xmax','ymin','ymax']
            b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),
                 float(xmlbox.find('ymax').text))
            print(image_id, cls, b)
            # 带入进行归一化操作
            # w = 宽, h = 高， b= bndbox的数组 = ['xmin','xmax','ymin','ymax']
            bb = convert((w, h), b)
            # bb 对应的是归一化后的(x,y,w,h)
            # 生成 calss x y w h 在label文件中
            out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')


# 返回当前工作目录
wd = getcwd()
print(wd)


for image_set in sets:
    '''
    对所有的文件数据集进行遍历
    做了两个工作：
　　　　１．将所有图片文件都遍历一遍，并且将其所有的全路径都写在对应的txt文件中去，方便定位
　　　　２．同时对所有的图片文件进行解析和转化，将其对应的bundingbox 以及类别的信息全部解析写到label 文件中去
    　　　　　最后再通过直接读取文件，就能找到对应的label 信息
    '''
    # 先找labels文件夹如果不存在则创建
    if not os.path.exists('data/labels/'):
        os.makedirs('data/labels/')
    # 读取在ImageSets/Main 中的train、test..等文件的内容
    # 包含对应的文件名称
    image_ids = open('data/ImageSets/%s.txt' % (image_set)).read().strip().split()
    # 打开对应的2012_train.txt 文件对其进行写入准备
    list_file = open('data/%s.txt' % (image_set), 'w')
    # 将对应的文件_id以及全路径写进去并换行
    for image_id in image_ids:
        list_file.write('data/images/%s.jpg\n' % (image_id))
        # 调用  year = 年份  image_id = 对应的文件名_id
        convert_annotation(image_id)
    # 关闭文件
    list_file.close()
```

- 分别运行makeTxt.py和voc_label.py。
- makeTxt.py主要是将数据集分类成训练数据集和测试数据集，默认train，val，test按照8：1：1的比例进行随机分类，运行后ImagesSets文件夹中会出现四个文件，主要是生成的训练数据集和测试数据集的图片名称，同时data目录下也会出现这四个文件，内容是训练数据集和测试数据集的图片路径。

![image-20210429000427755](方案.assets/image-20210429000427755.png)

<center style="color:#C0C0C0;text-decoration:underline">数据集结构</center>

![image-20210429000514574](方案.assets/image-20210429000514574.png)

<center style="color:#C0C0C0;text-decoration:underline">数据集labels结构</center>

- #### 数据集方面yaml文件修改

修改data目录下的coco.yaml

```python
# COCO 2017 dataset http://cocodataset.org
# Download command: bash yolov5/data/get_coco2017.sh
# Train command: python train.py --data ./data/coco.yaml
# Dataset should be placed next to yolov5 folder:
#   /parent_folder
#     /coco
#     /yolov5


# train and val datasets (image directory or *.txt file with image paths)
train: data/train.txt  # 118k images
val: data/val.txt  # 5k images
test: data/test.txt  # 20k images for submission to https://competitions.codalab.org/competitions/20794

# number of classes
nc: 3

# class names
names: ['', '',''] #上文提到的自己标注的数据集标记

# Print classes
# with open('data/coco.yaml') as f:
#   d = yaml.load(f, Loader=yaml.FullLoader)  # dict
#   for i, x in enumerate(d['names']):
#     print(i, x)
```

- #### 网络参数方面的yaml文件修改

  接着在models目录下的yolov5s.yaml文件进行修改，这里取决于你使用了哪个模型就去修改对于的文件，该项目中使用的是yolov5s模型。需要修改的代码如下：

  ```python
  # parameters
  nc: 3  # number of classes
  depth_multiple: 0.33  # model depth multiple
  width_multiple: 0.50  # layer channel multiple
  ```

- #### train.py中的一些参数修改

  在根目录中对train.py中的一些参数进行修改，主要用到的有这几个参数：–weights，–cfg，–data，–epochs，–batch-size，–img-size，–project

```python
parser = argparse.ArgumentParser()
# 加载的权重文件
parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')
# 模型配置文件，网络结构，使用修改好的yolov5m.yaml文件
parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')
# 数据集配置文件，数据集路径，类名等，使用数据集方面的coco.yaml文件
parser.add_argument('--data', type=str, default='data/coco.yaml', help='data.yaml path')
# 超参数文件
parser.add_argument('--hyp', type=str, default='data/hyp.scratch.yaml', help='hyperparameters path')
# 训练总轮次，1个epoch等于使用训练集中的全部样本训练一次，值越大模型越精确，训练时间也越长。
parser.add_argument('--epochs', type=int, default=300)
# 批次大小，一次训练所选取的样本数，显卡垃圾的话，就调小点
parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')
# 输入图片分辨率大小，nargs='+'表示参数可设置一个或多个
parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='[train, test] image sizes')
# 是否采用矩形训练，默认False，开启后可显著的减少推理时间
parser.add_argument('--rect', action='store_true', help='rectangular training')
# 接着打断训练上次的结果接着训练
parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')
# 不保存模型，默认False
parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')
# 不进行test，默认False
parser.add_argument('--notest', action='store_true', help='only test final epoch')
# 不自动调整anchor，默认False
parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')
# 是否进行超参数进化，默认False
parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')
# 谷歌云盘bucket，一般不会用到
parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
# 是否提前缓存图片到内存，以加快训练速度，默认False
parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')
# 选用加权图像进行训练
parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')
# 训练的设备，cpu；0(表示一个gpu设备cuda:0)；0,1,2,3(多个gpu设备)。值为空时，训练时默认使用计算机自带的显卡或CPU
parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
# 是否进行多尺度训练，默认False
parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')
# 数据集是否只有一个类别，默认False
parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')
# 是否使用adam优化器
parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')
# 是否使用跨卡同步BN,在DDP模式使用
parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
# gpu编号
parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')
# W&B记录的图像数，最大为100
parser.add_argument('--log-imgs', type=int, default=16, help='number of images for W&B logging, max 100')
# 记录最终训练的模型，即last.pt
parser.add_argument('--log-artifacts', action='store_true', help='log artifacts, i.e. final trained model')
# dataloader的最大worker数量
parser.add_argument('--workers', type=int, default=4, help='maximum number of dataloader workers')
# 训练结果所存放的路径，默认为runs/train
parser.add_argument('--project', default='runs/train', help='save to project/name')
# 训练结果所在文件夹的名称，默认为exp
parser.add_argument('--name', default='exp', help='save to project/name')
# 若现有的project/name存在，则不进行递增
parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
opt = parser.parse_args()
```

### 三，开始训练

- #### 在更改完所有配置之后运行命令(cmd已在当前目录下)

  ```python
  python train.py
  ```

  随后出现

  ```python
  Using torch 1.7.1+cu110 CUDA:0 (GeForce RTX 2060, 6144MB)
  
  Namespace(adam=False, batch_size=16, bucket='', cache_images=False, cfg='', data='data/coco.yaml', device='', epochs=300, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs\\train\\exp', single_cls=False, sync_bn=False, total_batch_size=16, weights='yolov5s.pt', workers=8, world_size=1)
  Start Tensorboard with "tensorboard --logdir runs/train", view at http://localhost:6006/
  Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}
  ```

  稍等片刻加载图片后进入训练

  ```python
  Analyzing anchors... anchors/target = 5.31, Best Possible Recall (BPR) = 1.0000
  Image sizes 640 train, 640 test
  Using 6 dataloader workers
  Logging results to runs\train\exp
  Starting training for 300 epochs...
  
  Epoch   gpu_mem       box       obj       cls     total   targets  img_size
  0/299      2.2G    0.1117   0.03632   0.02687    0.1749         2       640: 100%|██████████| 4/4 [00:17<00:00,4.35s/it]
  Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]
  all           6           0           0           0           0           0
  ```

- #### 漫长等待之后等待训练完成后进入YOLO文件夹目录下runs/train/exp，weight文件夹中两个.pt文件即为我们所训练出来的权重文件 

  - Best.pt文件为多次训练之后的最优权重
  - last.pt文件为最后一次训练得到出来的权重

### 四，使用训练出来的权重文件进行检测

- 有了训练好的权重后，就可以就行目标检测测试了。

  直接在根目录运行代码进行调试

  ```python
  python detect.py
  ```

  主要参数解释如下。我们平时用的话，主要用到的有这几个参数：–weights，–source，–conf-thres，–project。

  ```python
  parser = argparse.ArgumentParser()
  # 选用训练的权重，可用根目录下的yolov5s.pt，也可用runs/train/exp/weights/best.pt
  parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp/weights/best.pt', help='model.pt path(s)')
  # 检测数据，可以是图片/视频路径，也可以是'0'(电脑自带摄像头),也可以是rtsp等视频流
  parser.add_argument('--source', type=str, default='222.mp4', help='source')  # file/folder, 0 for webcam
  # 网络输入图片大小
  parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
  # 置信度阈值，检测到的对象属于特定类的概率
  parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')
  # 做nms的iou阈值
  parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')
  # 检测的设备，cpu；0(表示一个gpu设备cuda:0)；0,1,2,3(多个gpu设备)。值为空时，训练时默认使用计算机自带的显卡或CPU
  parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
  # 是否展示检测之后的图片/视频，默认False
  parser.add_argument('--view-img', action='store_true', help='display results')
  # 是否将检测的框坐标以txt文件形式保存，默认False
  parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
  # 是否将检测的labels以txt文件形式保存，默认False
  parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
  # 设置只保留某一部分类别，如0或者0 2 3
  parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
  # 进行nms是否也去除不同类别之间的框，默认False
  parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
  # 推理的时候进行多尺度，翻转等操作(TTA)推理
  parser.add_argument('--augment', action='store_true', help='augmented inference')
  # 如果为True，则对所有模型进行strip_optimizer操作，去除pt文件中的优化器等信息，默认为False
  parser.add_argument('--update', action='store_true', help='update all models')
  # 检测结果所存放的路径，默认为runs/detect
  parser.add_argument('--project', default='runs/detect', help='save results to project/name')
  # 检测结果所在文件夹的名称，默认为exp
  parser.add_argument('--name', default='exp', help='save results to project/name')
  # 若现有的project/name存在，则不进行递增
  parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
  opt = parser.parse_args()
  ```

  ![image-20210429004337115](方案.assets/image-20210429004337115.png)

<center style="color:#C0C0C0;text-decoration:underline">检测效果如图</center>

***<u>Yolov5目标检测至此结束</u>***